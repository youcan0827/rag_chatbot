#!/usr/bin/env python3
"""
å›½äº¤çœãƒ‡ãƒ¼ã‚¿ RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ - LangChainé¢¨å®Ÿè£…
ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¿ãƒ¼ãƒŸãƒŠãƒ«ç‰ˆRAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ
"""

import os
import PyPDF2
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
from openai import OpenAI
from dotenv import load_dotenv
import sys

# Hugging Face Tokenizersè­¦å‘Šã‚’æŠ‘åˆ¶
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

load_dotenv()

class LangChainStyleRAGChatbot:
    def __init__(self):
        self.documents = []
        self.embeddings = None
        self.index = None
        self.model = None
        self.openai_client = None
        
    def initialize(self):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        print("ğŸš€ ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¦ã„ã¾ã™...")
        
        # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«
        print("ğŸ“¥ ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # OpenRouterã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        api_key = os.getenv("OPENROUTER_API_KEY")
        if not api_key:
            print("âŒ OpenRouter APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        
        self.openai_client = OpenAI(
            api_key=api_key,
            base_url="https://openrouter.ai/api/v1"
        )
        print("âœ… ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†")
        return True
    
    def load_pdf(self, pdf_path):
        """PDFã‚’èª­ã¿è¾¼ã‚“ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        print(f"ğŸ“„ PDFã‚’èª­ã¿è¾¼ã¿ä¸­: {pdf_path}")
        
        if not os.path.exists(pdf_path):
            print("âŒ PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
        
        try:
            documents = []
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for i, page in enumerate(pdf_reader.pages):
                    text = page.extract_text()
                    if text.strip():
                        chunks = self.split_text(text, 500)
                        for j, chunk in enumerate(chunks):
                            documents.append({
                                'text': chunk,
                                'page': i + 1,
                                'chunk': j + 1,
                                'source': os.path.basename(pdf_path)
                            })
            
            self.documents = documents
            print(f"âœ… {len(documents)}å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
            return True
        except Exception as e:
            print(f"âŒ PDFèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def split_text(self, text, max_length=500):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šã—ãŸé•·ã•ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        sentences = text.split('ã€‚')
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk + sentence + "ã€‚") <= max_length:
                current_chunk += sentence + "ã€‚"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + "ã€‚"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return [chunk for chunk in chunks if len(chunk.strip()) > 10]
    
    def create_embeddings(self):
        """ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½œæˆã—ã¦FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰"""
        print("ğŸ”„ ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½œæˆä¸­...")
        
        try:
            texts = [doc['text'] for doc in self.documents]
            print(f"ğŸ“Š ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°å¯¾è±¡: {len(texts)}å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯")
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º
            if texts:
                print(f"ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ: {texts[0][:100]}...")
            
            self.embeddings = self.model.encode(texts, show_progress_bar=True)
            print(f"ğŸ“ ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°å½¢çŠ¶: {self.embeddings.shape}")
            
            # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            dimension = self.embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)
            
            # æ­£è¦åŒ–
            faiss.normalize_L2(self.embeddings)
            self.index.add(self.embeddings.astype('float32'))
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¢ºèª
            print(f"ğŸ“Š FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†…æ–‡æ›¸æ•°: {self.index.ntotal}")
            
            print(f"âœ… ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ä½œæˆå®Œäº†ï¼ˆæ¬¡å…ƒ: {dimension}ï¼‰")
            return True
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def search_similar_documents(self, query, k=3):
        """ã‚¯ã‚¨ãƒªã«é¡ä¼¼ã—ãŸæ–‡æ›¸ã‚’æ¤œç´¢"""
        try:
            query_embedding = self.model.encode([query])
            faiss.normalize_L2(query_embedding)
            
            scores, indices = self.index.search(query_embedding.astype('float32'), k)
            
            results = []
            print(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: æ¤œç´¢çµæœ (ã‚¯ã‚¨ãƒª: '{query}')")
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx < len(self.documents):
                    doc = self.documents[idx].copy()
                    doc['score'] = float(score)
                    print(f"   {i+1}. ã‚¹ã‚³ã‚¢: {score:.4f}, ãƒšãƒ¼ã‚¸{doc['page']}-{doc['chunk']}")
                    print(f"      ãƒ†ã‚­ã‚¹ãƒˆ: {doc['text'][:100]}...")
                    results.append(doc)
            
            return results
        except Exception as e:
            print(f"âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def generate_answer(self, query, context_docs):
        """OpenAI GPTã‚’ä½¿ç”¨ã—ã¦å›ç­”ã‚’ç”Ÿæˆ"""
        try:
            context = "\n\n".join([
                f"ã€ãƒšãƒ¼ã‚¸{doc['page']}-{doc['chunk']}ã€‘\n{doc['text']}" 
                for doc in context_docs
            ])
            
            prompt = f"""ä»¥ä¸‹ã®æ–‡æ›¸æƒ…å ±ã«åŸºã¥ã„ã¦ã€è³ªå•ã«æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
æ–‡æ›¸ã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒãªã„å ´åˆã¯ã€ã€Œæä¾›ã•ã‚ŒãŸæ–‡æ›¸ã«ã¯ãã®æƒ…å ±ãŒå«ã¾ã‚Œã¦ãŠã‚Šã¾ã›ã‚“ã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚

æ–‡æ›¸æƒ…å ±:
{context}

è³ªå•: {query}

å›ç­”:"""
            
            response = self.openai_client.chat.completions.create(
                model="openai/gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯å›½åœŸäº¤é€šçœã®æ–‡æ›¸ã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã™ã‚‹å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
        except Exception as e:
            return f"âŒ å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def chat(self):
        """ãƒãƒ£ãƒƒãƒˆãƒ«ãƒ¼ãƒ—"""
        print("\n" + "="*60)
        print("ğŸ”— å›½äº¤çœãƒ‡ãƒ¼ã‚¿ RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ (LangChainé¢¨)")
        print("="*60)
        print("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚çµ‚äº†ã™ã‚‹ã«ã¯ 'quit' ã¾ãŸã¯ 'exit' ã‚’å…¥åŠ›ã€‚")
        print("å‚ç…§æ–‡æ›¸ã‚’è¡¨ç¤ºã™ã‚‹ã«ã¯ 'verbose' ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚ªãƒ³ã«ã§ãã¾ã™ã€‚")
        print("-"*60)
        
        verbose = False
        
        while True:
            try:
                user_input = input("\nğŸ’¬ è³ªå•: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                    break
                
                if user_input.lower() == 'verbose':
                    verbose = not verbose
                    status = "ã‚ªãƒ³" if verbose else "ã‚ªãƒ•"
                    print(f"ğŸ“š è©³ç´°è¡¨ç¤º: {status}")
                    continue
                
                if not user_input:
                    continue
                
                print("\nğŸ” æ¤œç´¢ä¸­...")
                relevant_docs = self.search_similar_documents(user_input, k=3)
                
                # é¡ä¼¼åº¦ãŒä½ã™ãã‚‹å ´åˆã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆé–¾å€¤: 0.3ï¼‰
                min_similarity = 0.3
                filtered_docs = [doc for doc in relevant_docs if doc['score'] >= min_similarity]
                
                print(f"ğŸ“Š æ¤œç´¢çµæœ: {len(relevant_docs)}ä»¶ä¸­{len(filtered_docs)}ä»¶ãŒé–¾å€¤({min_similarity})ä»¥ä¸Š")
                
                if filtered_docs:
                    print("ğŸ’­ å›ç­”ç”Ÿæˆä¸­...")
                    answer = self.generate_answer(user_input, filtered_docs)
                    
                    print(f"\nğŸ¤– å›ç­”: {answer}")
                    
                    if verbose:
                        print("\nğŸ“š å‚ç…§ã—ãŸæ–‡æ›¸:")
                        print("-" * 40)
                        for i, doc in enumerate(filtered_docs, 1):
                            print(f"{i}. {doc['source']} - ãƒšãƒ¼ã‚¸{doc['page']}-{doc['chunk']} (é¡ä¼¼åº¦: {doc['score']:.3f})")
                            print(f"   {doc['text'][:150]}...")
                            print()
                else:
                    print("âŒ é–¢é€£ã™ã‚‹æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    print("ğŸ’¡ æ¤œç´¢ã•ã‚ŒãŸæ–‡æ›¸ã®é¡ä¼¼åº¦ãŒä½ã™ãã¾ã™ã€‚ã‚ˆã‚Šå…·ä½“çš„ãªè³ªå•ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚")
                    
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                break
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")

def main():
    print("ğŸ”— å›½äº¤çœãƒ‡ãƒ¼ã‚¿ RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ - LangChainé¢¨å®Ÿè£…")
    print("=" * 50)
    
    # OpenRouter APIã‚­ãƒ¼ç¢ºèª
    if not os.getenv("OPENROUTER_API_KEY"):
        print("âŒ OpenRouter APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        print("ğŸ’¡ .envãƒ•ã‚¡ã‚¤ãƒ«ã« OPENROUTER_API_KEY=your_api_key ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        return
    
    # ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆåˆæœŸåŒ–
    chatbot = LangChainStyleRAGChatbot()
    
    if not chatbot.initialize():
        return
    
    # PDFèª­ã¿è¾¼ã¿
    pdf_path = "/Users/yoshinomukanou/Downloads/å›½äº¤çœã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿/å›½äº¤çœâ‘ .pdf"
    if not chatbot.load_pdf(pdf_path):
        return
    
    # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ä½œæˆ
    if not chatbot.create_embeddings():
        return
    
    # ãƒãƒ£ãƒƒãƒˆé–‹å§‹
    chatbot.chat()

if __name__ == "__main__":
    main()