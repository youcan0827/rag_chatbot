#!/usr/bin/env python3
"""
å›½äº¤çœãƒ‡ãƒ¼ã‚¿ RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ - LlamaIndexç‰ˆ
LlamaIndexã‚’ä½¿ç”¨ã—ãŸRAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ
"""

import os
from dotenv import load_dotenv
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.readers.file import PyMuPDFReader
import logging
import sys

# Hugging Face Tokenizersè­¦å‘Šã‚’æŠ‘åˆ¶
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

# ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®š
logging.basicConfig(level=logging.WARNING)

load_dotenv()

class LlamaIndexRAGChatbot:
    def __init__(self):
        self.index = None
        self.query_engine = None
        
    def initialize(self):
        """LlamaIndexã®è¨­å®šã‚’åˆæœŸåŒ–"""
        print("ğŸš€ LlamaIndexã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¦ã„ã¾ã™...")
        
        try:
            # OpenAI API Keyç¢ºèª
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                print("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return False
            
            # LLMè¨­å®š
            llm = OpenAI(
                api_key=api_key,
                model="gpt-3.5-turbo",
                temperature=0.7
            )
            
            # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°è¨­å®šï¼ˆå¤šè¨€èªå¯¾å¿œï¼‰
            embed_model = HuggingFaceEmbedding(
                model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            )
            
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š
            Settings.llm = llm
            Settings.embed_model = embed_model
            Settings.chunk_size = 512
            Settings.chunk_overlap = 50
            
            print("âœ… LlamaIndexåˆæœŸåŒ–å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def load_and_index_pdf(self, pdf_path):
        """PDFã‚’èª­ã¿è¾¼ã‚“ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ"""
        print(f"ğŸ“„ PDFã‚’èª­ã¿è¾¼ã¿ä¸­: {pdf_path}")
        
        if not os.path.exists(pdf_path):
            print("âŒ PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
        
        try:
            # PDFãƒªãƒ¼ãƒ€ãƒ¼ä½œæˆ
            reader = PyMuPDFReader()
            
            # PDFã‚’èª­ã¿è¾¼ã¿
            documents = reader.load_data(file_path=pdf_path)
            print(f"âœ… {len(documents)}ãƒšãƒ¼ã‚¸ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            
            # ãƒãƒ¼ãƒ‰ãƒ‘ãƒ¼ã‚µãƒ¼è¨­å®š
            node_parser = SentenceSplitter(
                chunk_size=512,
                chunk_overlap=50
            )
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            print("ğŸ”„ ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆä¸­...")
            self.index = VectorStoreIndex.from_documents(
                documents,
                node_parser=node_parser,
                show_progress=True
            )
            
            # ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆ
            self.query_engine = self.index.as_query_engine(
                similarity_top_k=3,
                response_mode="tree_summarize"
            )
            
            print("âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ PDFå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def query(self, question):
        """è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’ç”Ÿæˆ"""
        if not self.query_engine:
            return "âŒ ã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        try:
            # ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§æ—¥æœ¬èªå›ç­”ã‚’æŒ‡å®š
            custom_prompt = f"""
ä»¥ä¸‹ã®æ–‡æ›¸æƒ…å ±ã«åŸºã¥ã„ã¦ã€è³ªå•ã«æ—¥æœ¬èªã§è©³ã—ãå›ç­”ã—ã¦ãã ã•ã„ã€‚
æ–‡æ›¸ã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒãªã„å ´åˆã¯ã€ã€Œæä¾›ã•ã‚ŒãŸæ–‡æ›¸ã«ã¯ãã®æƒ…å ±ãŒå«ã¾ã‚Œã¦ãŠã‚Šã¾ã›ã‚“ã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚

è³ªå•: {question}

å›ç­”:"""
            
            response = self.query_engine.query(custom_prompt)
            return str(response)
            
        except Exception as e:
            return f"âŒ å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def get_source_info(self, question):
        """ã‚½ãƒ¼ã‚¹æƒ…å ±ä»˜ãã§å›ç­”ã‚’å–å¾—"""
        if not self.query_engine:
            return "âŒ ã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“", []
        
        try:
            response = self.query_engine.query(question)
            
            # ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’æŠ½å‡º
            source_nodes = getattr(response, 'source_nodes', [])
            sources = []
            
            for node in source_nodes:
                if hasattr(node, 'node') and hasattr(node.node, 'metadata'):
                    metadata = node.node.metadata
                    text = node.node.text[:200] + "..." if len(node.node.text) > 200 else node.node.text
                    
                    sources.append({
                        'page': metadata.get('page_label', 'ä¸æ˜'),
                        'score': getattr(node, 'score', 0.0),
                        'text': text
                    })
            
            return str(response), sources
            
        except Exception as e:
            return f"âŒ å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}", []
    
    def chat(self):
        """ãƒãƒ£ãƒƒãƒˆãƒ«ãƒ¼ãƒ—"""
        print("\n" + "="*60)
        print("ğŸ¦™ å›½äº¤çœãƒ‡ãƒ¼ã‚¿ RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ (LlamaIndexç‰ˆ)")
        print("="*60)
        print("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚çµ‚äº†ã™ã‚‹ã«ã¯ 'quit' ã¾ãŸã¯ 'exit' ã‚’å…¥åŠ›ã€‚")
        print("è©³ç´°æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ã«ã¯ 'verbose' ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚ªãƒ³ã«ã§ãã¾ã™ã€‚")
        print("-"*60)
        
        verbose = False
        
        while True:
            try:
                user_input = input("\nğŸ’¬ è³ªå•: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                    break
                
                if user_input.lower() == 'verbose':
                    verbose = not verbose
                    status = "ã‚ªãƒ³" if verbose else "ã‚ªãƒ•"
                    print(f"ğŸ“š è©³ç´°è¡¨ç¤º: {status}")
                    continue
                
                if not user_input:
                    continue
                
                print("\nğŸ” LlamaIndexã§æ¤œç´¢ãƒ»å›ç­”ç”Ÿæˆä¸­...")
                
                if verbose:
                    answer, sources = self.get_source_info(user_input)
                    
                    print(f"\nğŸ¦™ å›ç­”: {answer}")
                    
                    if sources:
                        print("\nğŸ“š å‚ç…§ã—ãŸæ–‡æ›¸:")
                        print("-" * 40)
                        for i, source in enumerate(sources, 1):
                            print(f"{i}. ãƒšãƒ¼ã‚¸{source['page']} (ã‚¹ã‚³ã‚¢: {source['score']:.3f})")
                            print(f"   {source['text']}")
                            print()
                else:
                    answer = self.query(user_input)
                    print(f"\nğŸ¦™ å›ç­”: {answer}")
                    
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                break
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")

def main():
    print("ğŸ¦™ å›½äº¤çœãƒ‡ãƒ¼ã‚¿ RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ - LlamaIndexç‰ˆ")
    print("=" * 50)
    
    # OpenAI APIã‚­ãƒ¼ç¢ºèª
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        print("ğŸ’¡ .envãƒ•ã‚¡ã‚¤ãƒ«ã« OPENAI_API_KEY=your_api_key ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        return
    
    # ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆåˆæœŸåŒ–
    chatbot = LlamaIndexRAGChatbot()
    
    if not chatbot.initialize():
        return
    
    # PDFèª­ã¿è¾¼ã¿
    pdf_path = "/Users/yoshinomukanou/Downloads/å›½äº¤çœã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿/å›½äº¤çœâ‘ .pdf"
    if not chatbot.load_and_index_pdf(pdf_path):
        return
    
    # ãƒãƒ£ãƒƒãƒˆé–‹å§‹
    chatbot.chat()

if __name__ == "__main__":
    main()