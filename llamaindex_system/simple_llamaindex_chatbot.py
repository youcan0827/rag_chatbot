#!/usr/bin/env python3
"""
å›½äº¤çœãƒ‡ãƒ¼ã‚¿ RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ - ã‚·ãƒ³ãƒ—ãƒ«ãªLlamaIndexé¢¨å®Ÿè£…
ä¾å­˜é–¢ä¿‚ã‚’æœ€å°é™ã«ã—ãŸã‚·ãƒ³ãƒ—ãƒ«ç‰ˆ
"""

import os
import PyPDF2
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
from openai import OpenAI
from dotenv import load_dotenv

# Hugging Face Tokenizersè­¦å‘Šã‚’æŠ‘åˆ¶
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

load_dotenv()

class SimpleLlamaIndexChatbot:
    def __init__(self):
        self.documents = []
        self.embeddings = None
        self.index = None
        self.model = None
        self.openai_client = None
        
    def initialize(self):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ– - LlamaIndexé¢¨ã®çµ±åˆAPI"""
        print("ğŸ¦™ SimpleLlamaIndexã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¦ã„ã¾ã™...")
        
        try:
            # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«
            print("ğŸ“¥ ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            
            # OpenRouter APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
            api_key = os.getenv("OPENROUTER_API_KEY")
            if not api_key:
                print("âŒ OpenRouter APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return False
            
            self.openai_client = OpenAI(
                api_key=api_key,
                base_url="https://openrouter.ai/api/v1"
            )
            print("âœ… SimpleLlamaIndexåˆæœŸåŒ–å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def load_document(self, pdf_path):
        """æ–‡æ›¸èª­ã¿è¾¼ã¿ - LlamaIndexé¢¨ã®Document API"""
        print(f"ğŸ“„ PDFã‚’èª­ã¿è¾¼ã¿ä¸­: {pdf_path}")
        
        if not os.path.exists(pdf_path):
            print("âŒ PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
        
        try:
            documents = []
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for i, page in enumerate(pdf_reader.pages):
                    text = page.extract_text()
                    if text.strip():
                        # LlamaIndexé¢¨ã®ãƒãƒ£ãƒ³ã‚¯ãƒ³ã‚°
                        chunks = self.sentence_splitter(text, chunk_size=512, chunk_overlap=50)
                        for j, chunk in enumerate(chunks):
                            documents.append({
                                'text': chunk,
                                'metadata': {
                                    'page_label': str(i + 1),
                                    'chunk_id': j + 1,
                                    'source': os.path.basename(pdf_path)
                                }
                            })
            
            self.documents = documents
            print(f"âœ… {len(documents)}å€‹ã®ãƒãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¾ã—ãŸ")
            return True
            
        except Exception as e:
            print(f"âŒ PDFå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def sentence_splitter(self, text, chunk_size=512, chunk_overlap=50):
        """LlamaIndexé¢¨ã®SentenceSplitter"""
        sentences = text.split('ã€‚')
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk + sentence + "ã€‚") <= chunk_size:
                current_chunk += sentence + "ã€‚"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—å‡¦ç†
                if chunk_overlap > 0 and current_chunk:
                    overlap_text = current_chunk[-chunk_overlap:]
                    current_chunk = overlap_text + sentence + "ã€‚"
                else:
                    current_chunk = sentence + "ã€‚"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return [chunk for chunk in chunks if len(chunk.strip()) > 10]
    
    def create_vector_index(self):
        """ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ - LlamaIndexé¢¨ã®VectorStoreIndex"""
        print("ğŸ”„ VectorStoreIndexã‚’ä½œæˆä¸­...")
        
        try:
            texts = [doc['text'] for doc in self.documents]
            self.embeddings = self.model.encode(texts, show_progress_bar=True)
            
            # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            dimension = self.embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)
            
            # æ­£è¦åŒ–
            faiss.normalize_L2(self.embeddings)
            self.index.add(self.embeddings.astype('float32'))
            
            print(f"âœ… VectorStoreIndexä½œæˆå®Œäº†ï¼ˆæ¬¡å…ƒ: {dimension}ï¼‰")
            return True
            
        except Exception as e:
            print(f"âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def as_query_engine(self, similarity_top_k=3):
        """ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆ - LlamaIndexé¢¨ã®QueryEngine API"""
        self.similarity_top_k = similarity_top_k
        return self
    
    def query(self, question):
        """ã‚¯ã‚¨ãƒªå®Ÿè¡Œ - LlamaIndexé¢¨ã®Query API"""
        if not self.index:
            return "âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        try:
            # é¡ä¼¼æ–‡æ›¸æ¤œç´¢
            query_embedding = self.model.encode([question])
            faiss.normalize_L2(query_embedding)
            
            scores, indices = self.index.search(query_embedding.astype('float32'), self.similarity_top_k)
            
            # ã‚½ãƒ¼ã‚¹ãƒãƒ¼ãƒ‰ä½œæˆ
            source_nodes = []
            context_texts = []
            
            for score, idx in zip(scores[0], indices[0]):
                if idx < len(self.documents):
                    doc = self.documents[idx]
                    source_nodes.append({
                        'node': {
                            'text': doc['text'],
                            'metadata': doc['metadata']
                        },
                        'score': float(score)
                    })
                    context_texts.append(doc['text'])
            
            # Tree Summarizeé¢¨ã®å›ç­”ç”Ÿæˆ
            response = self.tree_summarize(question, context_texts)
            
            # LlamaIndexé¢¨ã®Response ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            return LlamaIndexResponse(response, source_nodes)
            
        except Exception as e:
            return f"âŒ ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def tree_summarize(self, question, context_texts):
        """Tree Summarizeé¢¨ã®å›ç­”ç”Ÿæˆ"""
        try:
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆçµ±åˆ
            combined_context = "\n\n".join([
                f"æ–‡æ›¸{i+1}: {text}" for i, text in enumerate(context_texts)
            ])
            
            # é«˜åº¦ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆLlamaIndexé¢¨ï¼‰
            prompt = f"""ã‚ãªãŸã¯å°‚é–€çš„ãªæ–‡æ›¸åˆ†æã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®æ–‡æ›¸æƒ…å ±ã‚’ç·åˆçš„ã«åˆ†æã—ã¦ã€è³ªå•ã«è©³ã—ãæ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

è¤‡æ•°ã®æ–‡æ›¸æƒ…å ±:
{combined_context}

è³ªå•: {question}

å›ç­”ã®æŒ‡é‡:
1. è¤‡æ•°ã®æ–‡æ›¸ã‹ã‚‰é–¢é€£ã™ã‚‹æƒ…å ±ã‚’çµ±åˆã—ã¦å›ç­”
2. æ–‡æ›¸ã«ãªã„æƒ…å ±ã¯æ¨æ¸¬ã›ãšã€ã€Œæä¾›ã•ã‚ŒãŸæ–‡æ›¸ã«ã¯è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€ã¨æ˜è¨˜
3. å¯èƒ½ãªé™ã‚Šå…·ä½“çš„ã§è©³ç´°ãªå›ç­”ã‚’æä¾›
4. æƒ…å ±æºãŒè¤‡æ•°ã‚ã‚‹å ´åˆã¯ã€ãã‚Œãã‚Œã®è¦³ç‚¹ã‚’æ•´ç†ã—ã¦èª¬æ˜

å›ç­”:"""
            
            response = self.openai_client.chat.completions.create(
                model="openai/gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯å›½åœŸäº¤é€šçœã®å°‚é–€æ–‡æ›¸ã‚’åˆ†æã™ã‚‹é«˜åº¦ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚è¤‡æ•°ã®æ–‡æ›¸ã‚’çµ±åˆã—ã¦åŒ…æ‹¬çš„ãªå›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            return f"âŒ å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def chat(self):
        """ãƒãƒ£ãƒƒãƒˆãƒ«ãƒ¼ãƒ—"""
        print("\n" + "="*60)
        print("ğŸ¦™ å›½äº¤çœãƒ‡ãƒ¼ã‚¿ RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ (SimpleLlamaIndexç‰ˆ)")
        print("="*60)
        print("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚çµ‚äº†ã™ã‚‹ã«ã¯ 'quit' ã¾ãŸã¯ 'exit' ã‚’å…¥åŠ›ã€‚")
        print("è©³ç´°æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ã«ã¯ 'verbose' ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚ªãƒ³ã«ã§ãã¾ã™ã€‚")
        print("-"*60)
        
        verbose = False
        
        while True:
            try:
                user_input = input("\nğŸ’¬ è³ªå•: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                    break
                
                if user_input.lower() == 'verbose':
                    verbose = not verbose
                    status = "ã‚ªãƒ³" if verbose else "ã‚ªãƒ•"
                    print(f"ğŸ“š è©³ç´°è¡¨ç¤º: {status}")
                    continue
                
                if not user_input:
                    continue
                
                print("\nğŸ” SimpleLlamaIndexã§æ¤œç´¢ãƒ»å›ç­”ç”Ÿæˆä¸­...")
                
                response = self.query(user_input)
                
                if isinstance(response, LlamaIndexResponse):
                    print(f"\nğŸ¦™ å›ç­”: {response.response}")
                    
                    if verbose and response.source_nodes:
                        print("\nğŸ“š å‚ç…§ã—ãŸã‚½ãƒ¼ã‚¹ãƒãƒ¼ãƒ‰:")
                        print("-" * 40)
                        for i, source in enumerate(response.source_nodes, 1):
                            metadata = source['node']['metadata']
                            print(f"{i}. {metadata['source']} - ãƒšãƒ¼ã‚¸{metadata['page_label']}-{metadata['chunk_id']} (ã‚¹ã‚³ã‚¢: {source['score']:.3f})")
                            print(f"   {source['node']['text'][:150]}...")
                            print()
                else:
                    print(f"\nğŸ¦™ å›ç­”: {response}")
                    
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                break
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")

class LlamaIndexResponse:
    """LlamaIndexé¢¨ã®Responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ"""
    def __init__(self, response_text, source_nodes):
        self.response = response_text
        self.source_nodes = source_nodes
    
    def __str__(self):
        return self.response

def main():
    print("ğŸ¦™ å›½äº¤çœãƒ‡ãƒ¼ã‚¿ RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ - SimpleLlamaIndexç‰ˆ")
    print("=" * 55)
    
    # OpenRouter APIã‚­ãƒ¼ç¢ºèª
    if not os.getenv("OPENROUTER_API_KEY"):
        print("âŒ OpenRouter APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        print("ğŸ’¡ .envãƒ•ã‚¡ã‚¤ãƒ«ã« OPENROUTER_API_KEY=your_api_key ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        return
    
    # ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆåˆæœŸåŒ–
    chatbot = SimpleLlamaIndexChatbot()
    
    if not chatbot.initialize():
        return
    
    # PDFèª­ã¿è¾¼ã¿
    pdf_path = "/Users/yoshinomukanou/Downloads/å›½äº¤çœã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿/å›½äº¤çœâ‘ .pdf"
    if not chatbot.load_document(pdf_path):
        return
    
    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    if not chatbot.create_vector_index():
        return
    
    # ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³è¨­å®š
    query_engine = chatbot.as_query_engine(similarity_top_k=3)
    
    # ãƒãƒ£ãƒƒãƒˆé–‹å§‹
    query_engine.chat()

if __name__ == "__main__":
    main()